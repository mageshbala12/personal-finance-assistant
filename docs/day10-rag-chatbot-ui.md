Day 10 â€” Integrate RAG into Streamlit UI

ğŸ“š Part 1 â€” What We Are Building Today
Today we take the RAG pipeline from Day 9 and wrap it in a beautiful, usable chat interface. By end of today you'll have a real working Finance Assistant that:
User uploads bank statement PDF/TXT
            â†“
App indexes it into ChromaDB
            â†“
User asks questions in chat
            â†“
RAG pipeline retrieves relevant chunks
            â†“
Gemini answers with YOUR data
            â†“
Sources shown below every answer

ğŸ“š Part 2 â€” UI Design Plan
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’° Personal Finance Assistant      â”‚
â”‚  Powered by RAG                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ SIDEBAR                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Upload Document                    â”‚
â”‚  [Browse Files]                     â”‚
â”‚                                     â”‚
â”‚  âœ… Document indexed!               â”‚
â”‚  Chunks: 4                          â”‚
â”‚  Source: statement.txt              â”‚
â”‚                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  About                              â”‚
â”‚  Stage 1: RAG                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MAIN CHAT AREA                     â”‚
â”‚                                     â”‚
â”‚  ğŸ¤– Hello! Upload your bank         â”‚
â”‚     statement to get started.       â”‚
â”‚                                     â”‚
â”‚  ğŸ‘¤ How much did I spend on         â”‚
â”‚     Zomato in January?              â”‚
â”‚                                     â”‚
â”‚  ğŸ¤– You spent â‚¹2,050 on Zomato:    â”‚
â”‚     â€¢ 03-Jan: â‚¹850                  â”‚
â”‚     â€¢ 12-Jan: â‚¹650                  â”‚
â”‚     â€¢ 20-Jan: â‚¹550                  â”‚
â”‚     ğŸ“„ Source: statement.txt        â”‚
â”‚                                     â”‚
â”‚  [Ask a question about              â”‚
â”‚   your finances...]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’» Part 3 â€” Build the RAG Chatbot UI
Step 1 â€” Create New File
In src/ folder create rag_chatbot.py and paste:
pythonimport streamlit as st
import sys
import os
import tempfile

# Add src to path so we can import our modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rag_pipeline import (
    get_llm,
    get_embedding_model,
    index_documents,
    load_vector_store,
    rag_query,
    CHROMA_DB_PATH
)

# â”€â”€ Page configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Personal Finance Assistant",
    page_icon="ğŸ’°",
    layout="wide"
)

# â”€â”€ Custom CSS for better appearance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .source-box {
        background-color: #f0f2f6;
        border-left: 3px solid #4CAF50;
        padding: 8px 12px;
        margin-top: 8px;
        border-radius: 4px;
        font-size: 0.85em;
        color: #555;
    }
    .chunk-box {
        background-color: #fff8e1;
        border-left: 3px solid #FF9800;
        padding: 8px 12px;
        margin-top: 4px;
        border-radius: 4px;
        font-size: 0.80em;
        color: #666;
    }
</style>
""", unsafe_allow_html=True)


# â”€â”€ Initialize session state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session_state():
    """
    Initialize all session state variables.
    Called once when app starts.
    """
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None

    if "llm" not in st.session_state:
        st.session_state.llm = get_llm()

    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = get_embedding_model()

    if "document_indexed" not in st.session_state:
        st.session_state.document_indexed = False

    if "indexed_filename" not in st.session_state:
        st.session_state.indexed_filename = None

    if "show_chunks" not in st.session_state:
        st.session_state.show_chunks = False


# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_sidebar():
    """
    Render the sidebar with document upload and settings.
    """
    with st.sidebar:
        st.title("ğŸ“ Document Manager")
        st.divider()

        # Document upload section
        st.subheader("Upload Financial Document")
        st.caption("Supported: PDF, TXT files")

        uploaded_file = st.file_uploader(
            label="Choose a file",
            type=["pdf", "txt"],
            help="Upload your bank statement or financial document"
        )

        # Process uploaded file
        if uploaded_file is not None:
            if uploaded_file.name != st.session_state.indexed_filename:
                with st.spinner("ğŸ“¥ Indexing document..."):
                    try:
                        # Save uploaded file temporarily
                        with tempfile.NamedTemporaryFile(
                            delete=False,
                            suffix=os.path.splitext(uploaded_file.name)[1]
                        ) as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            tmp_file_path = tmp_file.name

                        # Index the document
                        st.session_state.vector_store = index_documents(
                            tmp_file_path
                        )
                        st.session_state.document_indexed = True
                        st.session_state.indexed_filename = uploaded_file.name

                        # Clean up temp file
                        os.unlink(tmp_file_path)

                        # Add welcome message to chat
                        st.session_state.messages = []
                        st.session_state.messages.append({
                            "role"    : "assistant",
                            "content" : f"âœ… Successfully indexed **{uploaded_file.name}**! I'm ready to answer questions about your finances. What would you like to know?",
                            "sources" : [],
                            "chunks"  : []
                        })

                    except Exception as e:
                        st.error(f"âŒ Error indexing document: {str(e)}")

        # Show document status
        st.divider()
        st.subheader("ğŸ“Š Status")

        if st.session_state.document_indexed:
            st.success(f"âœ… Document loaded")
            st.info(f"ğŸ“„ {st.session_state.indexed_filename}")
        else:
            st.warning("âš ï¸ No document loaded")
            st.caption("Upload a document to start chatting")

        # Settings
        st.divider()
        st.subheader("âš™ï¸ Settings")
        st.session_state.show_chunks = st.toggle(
            "Show retrieved chunks",
            value=False,
            help="Show which parts of document were used to answer"
        )

        # About section
        st.divider()
        st.subheader("â„¹ï¸ About")
        st.caption("""
        **Personal Finance Assistant**
        Stage 1: RAG Pipeline

        Built with:
        - Google Gemini AI
        - LangChain
        - ChromaDB
        - Streamlit
        """)


# â”€â”€ Main chat area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_chat():
    """
    Render the main chat interface.
    """
    st.title("ğŸ’° Personal Finance Assistant")
    st.caption("Upload your bank statement and ask questions about your finances!")

    # Show welcome message if no document loaded
    if not st.session_state.document_indexed:
        st.info("""
        ğŸ‘‹ **Welcome to your Personal Finance Assistant!**

        To get started:
        1. Upload your bank statement (PDF or TXT) in the sidebar
        2. Wait for indexing to complete
        3. Start asking questions about your finances!

        **Example questions you can ask:**
        - How much did I spend on food this month?
        - What are my total SIP investments?
        - What is my closing balance?
        - When was my salary credited?
        """)
        return

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # Show sources if available
            if message.get("sources"):
                sources_text = ", ".join([
                    os.path.basename(s)
                    for s in message["sources"]
                ])
                st.markdown(
                    f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
                    unsafe_allow_html=True
                )

            # Show chunks if toggle is on
            if st.session_state.show_chunks and message.get("chunks"):
                with st.expander("ğŸ” View retrieved chunks"):
                    for i, (doc, score) in enumerate(message["chunks"]):
                        st.markdown(
                            f'<div class="chunk-box">'
                            f'<b>Chunk {i+1}</b> '
                            f'(score: {round(score, 4)})<br>'
                            f'{doc.page_content[:200]}...'
                            f'</div>',
                            unsafe_allow_html=True
                        )

    # Chat input
    if prompt := st.chat_input("Ask a question about your finances..."):

        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)

        # Save user message
        st.session_state.messages.append({
            "role"    : "user",
            "content" : prompt,
            "sources" : [],
            "chunks"  : []
        })

        # Get RAG response
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Searching your documents..."):
                result = rag_query(
                    prompt,
                    st.session_state.vector_store,
                    st.session_state.llm
                )

            # Display answer
            st.markdown(result["answer"])

            # Display sources
            if result["sources"]:
                sources_text = ", ".join([
                    os.path.basename(s)
                    for s in result["sources"]
                ])
                st.markdown(
                    f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
                    unsafe_allow_html=True
                )

            # Display chunks if toggle is on
            if st.session_state.show_chunks and result["chunks"]:
                with st.expander("ğŸ” View retrieved chunks"):
                    for i, (doc, score) in enumerate(result["chunks"]):
                        st.markdown(
                            f'<div class="chunk-box">'
                            f'<b>Chunk {i+1}</b> '
                            f'(score: {round(score, 4)})<br>'
                            f'{doc.page_content[:200]}...'
                            f'</div>',
                            unsafe_allow_html=True
                        )

        # Save assistant message
        st.session_state.messages.append({
            "role"    : "assistant",
            "content" : result["answer"],
            "sources" : result["sources"],
            "chunks"  : result["chunks"]
        })


# â”€â”€ Main app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    init_session_state()
    render_sidebar()
    render_chat()


if __name__ == "__main__":
    main()

Step 2 â€” Run It
bashstreamlit run src/rag_chatbot.py
```

Browser opens at `http://localhost:8501`

---

### Step 3 â€” Test It

1. In the sidebar click **"Browse Files"**
2. Upload `data/sample_statement.txt`
3. Wait for **"âœ… Successfully indexed"** message
4. Ask these questions in chat:
```
How much did I spend on Zomato?
What are my SIP investments?
What was my salary this month?
What is my closing balance?

Toggle "Show retrieved chunks" in sidebar to see which parts of document were used!


ğŸ’¾ Commit to GitHub
bashgit add .
git commit -m "feat: integrate RAG pipeline into Streamlit UI"
git push
```

---

## âœ… Day 10 Checklist
- [ ] Understand UI design plan
- [ ] Created `src/rag_chatbot.py`
- [ ] App runs locally in browser
- [ ] Document upload works
- [ ] Indexing completes successfully
- [ ] Chat answers questions correctly
- [ ] Sources shown below answers
- [ ] Chunks toggle works
- [ ] Committed and pushed to GitHub

---

## ğŸ—ºï¸ RAG Pipeline â€” Complete!
```
âœ… Step 1: Load Document       â† Day 5
âœ… Step 2: Preprocess Text     â† Day 5
âœ… Step 3: Split into Chunks   â† Day 6
âœ… Step 4: Create Embeddings   â† Day 7
âœ… Step 5: Store in ChromaDB   â† Day 8
âœ… Step 6: Query & Retrieve    â† Day 9
âœ… Step 7: Generate Answer     â† Day 9
âœ… Step 8: Integrate into UI   â† Done today!

ğŸ”œ Day 11 Preview
Deploy this RAG version to Render production:

Update Render configuration
Handle ChromaDB in production environment
Test live app with real documents
Your RAG Finance Assistant goes live!

One more day and RAG is in production! ğŸ’ª
Go ahead and run â€” let me know what you see or any errors! ğŸš€I am getting below error when i tried to upload the pdf file.....Error indexing document: Error embedding content (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: __https://ai.google.dev/gemini-api/docs/rate-limits__. To monitor your current usage, head to: __https://ai.dev/rate-limit__. \n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 100, model: gemini-embedding-1.0\nPlease retry in 30.9059074s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': '__https://ai.google.dev/gemini-api/docs/rate-limits__'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-embedding-1.0'}, 'quotaValue': '100'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '30s'}]}}Show more8:20 PMThis is a rate limit error â€” not a code error! ğŸ¯
You're on the free tier which allows only 100 embedding requests per minute. Your PDF likely has many chunks that need embedding all at once, exceeding this limit.

Understanding the Problem
Free Tier Limit:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
100 embedding requests per minute

Your PDF chunks â†’ each chunk = 1 embedding request
If PDF has 150 chunks â†’ 150 requests â†’ exceeds 100 limit â†’ âŒ

Fix â€” Add Rate Limiting to Vector Store
Open src/vector_store.py and find the create_vector_store() function.
Current code:
pythonvector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory=CHROMA_DB_PATH,
    collection_name=COLLECTION_NAME
)
Replace with:
pythonimport time

# Process chunks in batches to avoid rate limit
BATCH_SIZE = 50  # 50 chunks per batch
BATCH_DELAY = 65  # wait 65 seconds between batches

if len(chunks) <= BATCH_SIZE:
    # Small document â€” process all at once
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME
    )
else:
    # Large document â€” process in batches
    print(f"ğŸ“¦ Large document detected â€” processing in batches of {BATCH_SIZE}")

    # First batch creates the store
    first_batch = chunks[:BATCH_SIZE]
    vector_store = Chroma.from_documents(
        documents=first_batch,
        embedding=embedding_model,
        persist_directory=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME
    )
    print(f"âœ… Batch 1 done ({len(first_batch)} chunks)")

    # Remaining batches add to existing store
    remaining = chunks[BATCH_SIZE:]
    batch_num = 2

    while remaining:
        print(f"â³ Waiting {BATCH_DELAY}s before next batch...")
        time.sleep(BATCH_DELAY)

        batch = remaining[:BATCH_SIZE]
        remaining = remaining[BATCH_SIZE:]

        vector_store.add_documents(batch)
        print(f"âœ… Batch {batch_num} done ({len(batch)} chunks)")
        batch_num += 1

Also Add Delay in embeddings_manager.py
Open src/embeddings_manager.py and find embed_chunks(). Change:
pythontime.sleep(1)
To:
pythontime.sleep(2)

Quick Alternative â€” Reduce Chunk Count
If you don't want to wait, use a larger chunk size so fewer chunks are created. Open src/text_chunker.py and update default values:
pythondef create_chunks(documents, chunk_size=800, chunk_overlap=100):
Larger chunks = fewer total chunks = fewer embedding requests = stays under 100 limit.

Which Fix to Choose?
SituationFixSmall TXT file (our sample_statement.txt)Should work fine â€” try again after 1 minuteLarge PDF (10+ pages)Use batch processing fixWant quickest solutionIncrease chunk size to 800

Try Again
Wait 60 seconds then try uploading again. The rate limit resets every minute.
For our sample_statement.txt it should work fine since it only creates 4 chunks â€” well under the 100 limit. The issue likely happened because we ran multiple scripts today that used up the quota.
Let me know what happens after waiting 60 seconds! ğŸš€Provide me the day 10 documentation with detailed code level explanation8:29 PMğŸ“˜ Day 10 Documentation â€” RAG Chatbot UI & Rate Limiting
Personal Finance Assistant Project

ğŸ¯ Objective
Integrate the complete RAG pipeline into a Streamlit web interface with document upload, chat functionality, source display, and chunk inspection. Handle API rate limiting for production reliability.

ğŸ“š Part 1 â€” Key Concepts
What We Built Today
User uploads bank statement PDF/TXT
            â†“
App indexes it into ChromaDB
            â†“
User asks questions in chat
            â†“
RAG pipeline retrieves relevant chunks
            â†“
Gemini answers with YOUR data
            â†“
Sources shown below every answer
UI Design Plan
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’° Personal Finance Assistant      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ SIDEBAR      â”‚  MAIN CHAT AREA  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚                  â”‚
â”‚  Upload Document â”‚  Chat messages   â”‚
â”‚  [Browse Files]  â”‚  with sources    â”‚
â”‚                  â”‚                  â”‚
â”‚  âœ… Doc loaded   â”‚  [Ask question]  â”‚
â”‚  âš™ï¸ Settings     â”‚                  â”‚
â”‚  â„¹ï¸ About        â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Rate Limiting â€” Why It Happened
Free Tier Limit: 100 embedding requests per minute

Large PDF â†’ many chunks â†’ each chunk = 1 request
100+ chunks in one minute â†’ RESOURCE_EXHAUSTED error âŒ

Fix: Process chunks in batches with delay between batches âœ…

ğŸ’» Part 2 â€” Code Created
File: src/rag_chatbot.py
pythonimport streamlit as st
import sys
import os
import tempfile

# Add src to path so we can import our modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rag_pipeline import (
    get_llm,
    get_embedding_model,
    index_documents,
    load_vector_store,
    rag_query,
    CHROMA_DB_PATH
)

# â”€â”€ Page configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Personal Finance Assistant",
    page_icon="ğŸ’°",
    layout="wide"
)

# â”€â”€ Custom CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .source-box {
        background-color: #f0f2f6;
        border-left: 3px solid #4CAF50;
        padding: 8px 12px;
        margin-top: 8px;
        border-radius: 4px;
        font-size: 0.85em;
        color: #555;
    }
    .chunk-box {
        background-color: #fff8e1;
        border-left: 3px solid #FF9800;
        padding: 8px 12px;
        margin-top: 4px;
        border-radius: 4px;
        font-size: 0.80em;
        color: #666;
    }
</style>
""", unsafe_allow_html=True)


# â”€â”€ Initialize session state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    if "llm" not in st.session_state:
        st.session_state.llm = get_llm()
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = get_embedding_model()
    if "document_indexed" not in st.session_state:
        st.session_state.document_indexed = False
    if "indexed_filename" not in st.session_state:
        st.session_state.indexed_filename = None
    if "show_chunks" not in st.session_state:
        st.session_state.show_chunks = False


# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_sidebar():
    with st.sidebar:
        st.title("ğŸ“ Document Manager")
        st.divider()

        st.subheader("Upload Financial Document")
        st.caption("Supported: PDF, TXT files")

        uploaded_file = st.file_uploader(
            label="Choose a file",
            type=["pdf", "txt"],
            help="Upload your bank statement or financial document"
        )

        if uploaded_file is not None:
            if uploaded_file.name != st.session_state.indexed_filename:
                with st.spinner("ğŸ“¥ Indexing document..."):
                    try:
                        with tempfile.NamedTemporaryFile(
                            delete=False,
                            suffix=os.path.splitext(uploaded_file.name)[1]
                        ) as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            tmp_file_path = tmp_file.name

                        st.session_state.vector_store = index_documents(
                            tmp_file_path
                        )
                        st.session_state.document_indexed = True
                        st.session_state.indexed_filename = uploaded_file.name

                        os.unlink(tmp_file_path)

                        st.session_state.messages = []
                        st.session_state.messages.append({
                            "role"    : "assistant",
                            "content" : f"âœ… Successfully indexed **{uploaded_file.name}**! I'm ready to answer questions about your finances. What would you like to know?",
                            "sources" : [],
                            "chunks"  : []
                        })

                    except Exception as e:
                        st.error(f"âŒ Error indexing document: {str(e)}")

        st.divider()
        st.subheader("ğŸ“Š Status")

        if st.session_state.document_indexed:
            st.success(f"âœ… Document loaded")
            st.info(f"ğŸ“„ {st.session_state.indexed_filename}")
        else:
            st.warning("âš ï¸ No document loaded")
            st.caption("Upload a document to start chatting")

        st.divider()
        st.subheader("âš™ï¸ Settings")
        st.session_state.show_chunks = st.toggle(
            "Show retrieved chunks",
            value=False,
            help="Show which parts of document were used to answer"
        )

        st.divider()
        st.subheader("â„¹ï¸ About")
        st.caption("""
        **Personal Finance Assistant**
        Stage 1: RAG Pipeline

        Built with:
        - Google Gemini AI
        - LangChain
        - ChromaDB
        - Streamlit
        """)


# â”€â”€ Main chat area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_chat():
    st.title("ğŸ’° Personal Finance Assistant")
    st.caption("Upload your bank statement and ask questions about your finances!")

    if not st.session_state.document_indexed:
        st.info("""
        ğŸ‘‹ **Welcome to your Personal Finance Assistant!**

        To get started:
        1. Upload your bank statement (PDF or TXT) in the sidebar
        2. Wait for indexing to complete
        3. Start asking questions about your finances!

        **Example questions:**
        - How much did I spend on food this month?
        - What are my total SIP investments?
        - What is my closing balance?
        - When was my salary credited?
        """)
        return

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            if message.get("sources"):
                sources_text = ", ".join([
                    os.path.basename(s)
                    for s in message["sources"]
                ])
                st.markdown(
                    f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
                    unsafe_allow_html=True
                )

            if st.session_state.show_chunks and message.get("chunks"):
                with st.expander("ğŸ” View retrieved chunks"):
                    for i, (doc, score) in enumerate(message["chunks"]):
                        st.markdown(
                            f'<div class="chunk-box">'
                            f'<b>Chunk {i+1}</b> '
                            f'(score: {round(score, 4)})<br>'
                            f'{doc.page_content[:200]}...'
                            f'</div>',
                            unsafe_allow_html=True
                        )

    if prompt := st.chat_input("Ask a question about your finances..."):

        with st.chat_message("user"):
            st.markdown(prompt)

        st.session_state.messages.append({
            "role"    : "user",
            "content" : prompt,
            "sources" : [],
            "chunks"  : []
        })

        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Searching your documents..."):
                result = rag_query(
                    prompt,
                    st.session_state.vector_store,
                    st.session_state.llm
                )

            st.markdown(result["answer"])

            if result["sources"]:
                sources_text = ", ".join([
                    os.path.basename(s)
                    for s in result["sources"]
                ])
                st.markdown(
                    f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
                    unsafe_allow_html=True
                )

            if st.session_state.show_chunks and result["chunks"]:
                with st.expander("ğŸ” View retrieved chunks"):
                    for i, (doc, score) in enumerate(result["chunks"]):
                        st.markdown(
                            f'<div class="chunk-box">'
                            f'<b>Chunk {i+1}</b> '
                            f'(score: {round(score, 4)})<br>'
                            f'{doc.page_content[:200]}...'
                            f'</div>',
                            unsafe_allow_html=True
                        )

        st.session_state.messages.append({
            "role"    : "assistant",
            "content" : result["answer"],
            "sources" : result["sources"],
            "chunks"  : result["chunks"]
        })


# â”€â”€ Main app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    init_session_state()
    render_sidebar()
    render_chat()


if __name__ == "__main__":
    main()
Updated: src/vector_store.py â€” Batch Processing
Added batch processing to create_vector_store() to handle rate limits:
pythonimport time

BATCH_SIZE  = 50   # chunks per batch
BATCH_DELAY = 65   # seconds between batches

if len(chunks) <= BATCH_SIZE:
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME
    )
else:
    print(f"ğŸ“¦ Large document â€” processing in batches of {BATCH_SIZE}")

    first_batch  = chunks[:BATCH_SIZE]
    vector_store = Chroma.from_documents(
        documents=first_batch,
        embedding=embedding_model,
        persist_directory=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME
    )
    print(f"âœ… Batch 1 done ({len(first_batch)} chunks)")

    remaining = chunks[BATCH_SIZE:]
    batch_num = 2

    while remaining:
        print(f"â³ Waiting {BATCH_DELAY}s before next batch...")
        time.sleep(BATCH_DELAY)
        batch     = remaining[:BATCH_SIZE]
        remaining = remaining[BATCH_SIZE:]
        vector_store.add_documents(batch)
        print(f"âœ… Batch {batch_num} done ({len(batch)} chunks)")
        batch_num += 1

ğŸ” Part 3 â€” Detailed Code Explanation
Imports and Path Setup
pythonimport streamlit as st
import sys
import os
import tempfile

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
ImportPurposestreamlitUI frameworksysSystem operations â€” modify Python pathosFile and folder operationstempfileCreate temporary files for uploaded documents
sys.path.append() â€” tells Python where to find our modules:
pythonsys.path.append(os.path.dirname(os.path.abspath(__file__)))
__file__ â†’ current file's path (src/rag_chatbot.py)
os.path.abspath() â†’ converts to absolute path
os.path.dirname() â†’ gets the folder (src/)
sys.path.append() â†’ adds src/ to Python's search path
Why needed? When Streamlit runs rag_chatbot.py, Python may not know where rag_pipeline.py, vector_store.py etc. are located. This line explicitly tells Python to look in the src/ folder.

Custom CSS
pythonst.markdown("""
<style>
    .source-box {
        background-color: #f0f2f6;
        border-left: 3px solid #4CAF50;
        padding: 8px 12px;
        border-radius: 4px;
        font-size: 0.85em;
    }
    .chunk-box {
        background-color: #fff8e1;
        border-left: 3px solid #FF9800;
        ...
    }
</style>
""", unsafe_allow_html=True)
st.markdown() with unsafe_allow_html=True â€” injects raw HTML and CSS into the Streamlit page. Normally Streamlit blocks HTML for security. unsafe_allow_html=True explicitly allows it.
CSS classes created:
ClassColorUsed For.source-boxGreen left borderShow answer sources.chunk-boxOrange left borderShow retrieved chunks
border-left: 3px solid #4CAF50 â†’ green vertical line on left side â€” visual indicator.
border-radius: 4px â†’ slightly rounded corners.
font-size: 0.85em â†’ slightly smaller than normal text. em is relative unit â€” 0.85em = 85% of parent font size.

init_session_state() Function
pythondef init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    if "llm" not in st.session_state:
        st.session_state.llm = get_llm()
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = get_embedding_model()
    if "document_indexed" not in st.session_state:
        st.session_state.document_indexed = False
    if "indexed_filename" not in st.session_state:
        st.session_state.indexed_filename = None
    if "show_chunks" not in st.session_state:
        st.session_state.show_chunks = False
Why check if "key" not in st.session_state?
Streamlit reruns the entire file on every interaction. Without this check, every rerun would reset all variables to their initial values â€” losing your chat history, loaded document, everything.
The check means: "Only initialize if not already set."
Session state variables explained:
VariableTypePurposemessagesListChat history â€” all messagesvector_storeObjectChromaDB instance â€” loaded documentllmObjectGemini LLM instanceembedding_modelObjectGoogle embedding model instancedocument_indexedBooleanIs a document loaded? Controls UI stateindexed_filenameStringName of loaded file â€” prevents re-indexing same fileshow_chunksBooleanToggle to show/hide retrieved chunks
Why store llm and embedding_model in session state?
Creating these objects makes API connections. If we recreated them on every Streamlit rerun it would be slow and wasteful. Storing in session state creates them once and reuses them throughout the session.

render_sidebar() Function
File uploader:
pythonuploaded_file = st.file_uploader(
    label="Choose a file",
    type=["pdf", "txt"],
    help="Upload your bank statement or financial document"
)
st.file_uploader() â†’ Streamlit's built in file upload widget.
type=["pdf", "txt"] â†’ restricts to only PDF and TXT files.
help= â†’ tooltip shown on hover.
Returns a file object or None if no file selected.
Prevent re-indexing same file:
pythonif uploaded_file is not None:
    if uploaded_file.name != st.session_state.indexed_filename:
Two checks:

uploaded_file is not None â†’ a file was actually uploaded
uploaded_file.name != st.session_state.indexed_filename â†’ it's a different file from what's already indexed

Why second check? Streamlit reruns on every interaction. Without this check, every time user clicks anywhere the app would re-index the already loaded document â€” very slow and wastes API quota.
Temporary file handling:
pythonwith tempfile.NamedTemporaryFile(
    delete=False,
    suffix=os.path.splitext(uploaded_file.name)[1]
) as tmp_file:
    tmp_file.write(uploaded_file.getvalue())
    tmp_file_path = tmp_file.name
Why temporary files? Streamlit's uploaded file is an in-memory object â€” not a real file on disk. Our load_document() function needs a real file path. So we:

Create a temporary file on disk
Write uploaded content to it
Get its path
Use path for indexing
Delete it when done

tempfile.NamedTemporaryFile() â†’ creates temp file in system's temp folder.
delete=False â†’ don't auto-delete when closed â€” we need to use it after.
suffix=os.path.splitext(uploaded_file.name)[1] â†’ keeps original extension (.pdf or .txt).
uploaded_file.getvalue() â†’ gets raw bytes of uploaded file.
tmp_file.name â†’ gets the temp file's path on disk.
Cleanup:
pythonos.unlink(tmp_file_path)
os.unlink() â†’ deletes a file. Cleans up temp file after indexing is done. Good practice â€” don't leave temp files behind.
Reset chat on new document:
pythonst.session_state.messages = []
st.session_state.messages.append({
    "role"    : "assistant",
    "content" : f"âœ… Successfully indexed **{uploaded_file.name}**!...",
    "sources" : [],
    "chunks"  : []
})
When a new document is uploaded we clear previous chat history and add a fresh welcome message. Prevents confusion from mixing answers from different documents.
Status indicators:
pythonif st.session_state.document_indexed:
    st.success(f"âœ… Document loaded")    # Green box
    st.info(f"ğŸ“„ {st.session_state.indexed_filename}")  # Blue box
else:
    st.warning("âš ï¸ No document loaded") # Yellow box
Streamlit's colored message boxes:
FunctionColorUsest.success()GreenPositive statusst.info()BlueInformationalst.warning()YellowCautionst.error()RedError
Chunks toggle:
pythonst.session_state.show_chunks = st.toggle(
    "Show retrieved chunks",
    value=False,
    help="Show which parts of document were used"
)
st.toggle() â†’ on/off switch widget. Returns True or False. Stored in session state so it persists across reruns.

render_chat() Function
Guard clause â€” no document loaded:
pythonif not st.session_state.document_indexed:
    st.info("""...""")
    return
return â†’ exits function early. If no document is loaded, show instructions and stop. Don't render the chat interface at all. This pattern is called a guard clause â€” check condition early, exit if not met.
Displaying chat history:
pythonfor message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        if message.get("sources"):
            sources_text = ", ".join([
                os.path.basename(s)
                for s in message["sources"]
            ])
            st.markdown(
                f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
                unsafe_allow_html=True
            )
message.get("sources") â†’ safely gets sources list. Returns None if key doesn't exist â€” no crash.
os.path.basename(s) â†’ extracts just filename from full path:
python"data/sample_statement.txt" â†’ "sample_statement.txt"
Cleaner display â€” users don't need to see full path.
", ".join([...]) â†’ joins list of filenames with comma separator.
Custom HTML for source box:
pythonst.markdown(
    f'<div class="source-box">ğŸ“„ Source: {sources_text}</div>',
    unsafe_allow_html=True
)
Uses our CSS class .source-box defined at the top. Creates the green-bordered source indicator below each answer.
Chunks expander:
pythonwith st.expander("ğŸ” View retrieved chunks"):
    for i, (doc, score) in enumerate(message["chunks"]):
        st.markdown(
            f'<div class="chunk-box">'
            f'<b>Chunk {i+1}</b> (score: {round(score, 4)})<br>'
            f'{doc.page_content[:200]}...'
            f'</div>',
            unsafe_allow_html=True
        )
st.expander() â†’ collapsible section. Hidden by default, user clicks to expand. Perfect for optional details like chunks that most users won't need.
doc.page_content[:200] â†’ first 200 characters of chunk. Enough to see what was retrieved without overwhelming the UI.
Processing new user input:
pythonif prompt := st.chat_input("Ask a question about your finances..."):
    with st.chat_message("user"):
        st.markdown(prompt)

    st.session_state.messages.append({
        "role"    : "user",
        "content" : prompt,
        "sources" : [],
        "chunks"  : []
    })

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” Searching your documents..."):
            result = rag_query(
                prompt,
                st.session_state.vector_store,
                st.session_state.llm
            )
        st.markdown(result["answer"])
st.spinner("ğŸ” Searching...") â†’ shows loading animation while RAG pipeline runs. Nested inside st.chat_message("assistant") so it appears in the right place in chat.
Message dictionary structure:
python{
    "role"    : "user" or "assistant",
    "content" : "the message text",
    "sources" : ["data/statement.txt"],   # for assistant only
    "chunks"  : [(doc, score), ...]       # for assistant only
}
We store sources and chunks with each assistant message so they can be redisplayed correctly when Streamlit reruns and redraws the chat history.

main() Function
pythondef main():
    init_session_state()
    render_sidebar()
    render_chat()

if __name__ == "__main__":
    main()
Clean entry point â€” three functions called in order:

Initialize state
Render sidebar
Render main chat area

Why wrap in main()? Keeps code organized. Clear entry point. Easy to add more sections later (e.g., render_header(), render_footer()).

Rate Limiting Fix â€” Batch Processing
pythonBATCH_SIZE  = 50
BATCH_DELAY = 65

if len(chunks) <= BATCH_SIZE:
    # Small document â€” process all at once
    vector_store = Chroma.from_documents(...)
else:
    # Large document â€” process in batches
    first_batch  = chunks[:BATCH_SIZE]
    vector_store = Chroma.from_documents(
        documents=first_batch, ...
    )

    remaining = chunks[BATCH_SIZE:]
    batch_num = 2

    while remaining:
        time.sleep(BATCH_DELAY)
        batch     = remaining[:BATCH_SIZE]
        remaining = remaining[BATCH_SIZE:]
        vector_store.add_documents(batch)
        batch_num += 1
List slicing for batches:
pythonfirst_batch = chunks[:BATCH_SIZE]    # first 50 chunks
remaining   = chunks[BATCH_SIZE:]    # everything after 50
chunks[:50] â†’ indices 0 to 49 (50 items)
chunks[50:] â†’ indices 50 onwards (everything remaining)
while remaining loop:
pythonwhile remaining:          # loop while list is not empty
    batch     = remaining[:BATCH_SIZE]   # take next 50
    remaining = remaining[BATCH_SIZE:]   # remove those 50
    vector_store.add_documents(batch)    # add to existing store
```

`while remaining` â†’ in Python a non-empty list is `True`, empty list is `False`. Loop continues until `remaining` is empty.

`vector_store.add_documents(batch)` â†’ adds new chunks to **existing** ChromaDB store without recreating it.

**Why 65 seconds delay?**
Rate limit is 100 requests per minute. We wait 65 seconds (slightly more than 60) to ensure the minute window fully resets before sending next batch.

---

## ğŸ“Š Part 4 â€” Actual Results
```
App running at: http://localhost:8501

âœ… Document upload working
âœ… Indexing completes with spinner feedback
âœ… Chat answers questions correctly
âœ… Sources shown below each answer
âœ… Chunks toggle shows retrieved context
âœ… Rate limit handled with batch processing
```

**Test questions and results:**

| Question | Result |
|----------|--------|
| How much did I spend on Zomato? | âœ… â‚¹2,050 with breakdown |
| What are my SIP investments? | âœ… â‚¹8,000 total |
| What was my salary? | âœ… â‚¹85,000 on 05-Jan |
| What is my closing balance? | âœ… â‚¹1,13,551 |

---

## ğŸ—ºï¸ RAG Pipeline â€” Complete!
```
âœ… Step 1: Load Document       â† Day 5
âœ… Step 2: Preprocess Text     â† Day 5
âœ… Step 3: Split into Chunks   â† Day 6
âœ… Step 4: Create Embeddings   â† Day 7
âœ… Step 5: Store in ChromaDB   â† Day 8
âœ… Step 6: Query & Retrieve    â† Day 9
âœ… Step 7: Generate Answer     â† Day 9
âœ… Step 8: Integrate into UI   â† Done today!

ğŸ’¡ Key Python Concepts Learned
ConceptExampleMeaningsys.path.append()Add folder to Python search pathFind modules in custom locationsGuard clauseif not condition: returnExit function early if condition not mettempfileNamedTemporaryFile()Create temporary file on diskos.unlink()Delete a fileClean up temp filesos.path.basename()Extract filename from path"data/file.txt" â†’ "file.txt"unsafe_allow_htmlst.markdown(..., unsafe_allow_html=True)Inject custom HTML/CSS into Streamlitst.expander()Collapsible sectionHide optional detailsst.toggle()On/off switch widgetUser controlled boolean settingList slicing for batcheschunks[:50], chunks[50:]Split list into batcheswhile remainingLoop until list emptyProcess all batchesvector_store.add_documents()Add to existing ChromaDBExtend without recreating

âš ï¸ Issues Faced & Solutions
IssueSolutionRESOURCE_EXHAUSTED 429 rate limit errorAdded batch processing with 65 second delay between batches of 50 chunksModules not found when running StreamlitAdded sys.path.append(os.path.dirname(os.path.abspath(__file__)))Same document re-indexed on every rerunAdded uploaded_file.name != st.session_state.indexed_filename check

âœ… Day 10 Checklist

 Understand UI design with sidebar and main chat area
 Understand temporary file handling for uploads
 Understand guard clauses
 Understand rate limiting and batch processing
 Created src/rag_chatbot.py
 App runs locally in browser
 Document upload and indexing works
 Chat answers questions correctly with sources
 Chunks toggle works
 Rate limit handled with batch processing
 Committed and pushed to GitHub