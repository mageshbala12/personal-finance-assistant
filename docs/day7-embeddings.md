ğŸ“˜ Day 7 Documentation â€” Embeddings
Personal Finance Assistant Project

ğŸ¯ Objective
Understand embeddings deeply, convert text chunks into vectors using Google's embedding model, and prove that similar texts produce similar embeddings.

ğŸ“š Part 1 â€” Key Concepts
What are Embeddings?
Embeddings convert text into lists of numbers (vectors) that capture meaning â€” not just characters.
Why not simple number mapping?
Simple approach (wrong):
"Zomato" â†’ 1
"Swiggy" â†’ 2
"Salary" â†’ 3
Problem: Numbers have no meaningful relationship to content
Embedding approach (correct):
"Zomato food order"     â†’ [0.2, 0.8, 0.1, 0.9, 0.3, ...]
"Swiggy food delivery"  â†’ [0.21, 0.79, 0.12, 0.88, 0.31, ...]
"Monthly salary credit" â†’ [0.9, 0.1, 0.7, 0.2, 0.8, ...]

Zomato and Swiggy â†’ numerically CLOSE â†’ similar meaning âœ…
Salary â†’ numerically FAR from both â†’ different meaning âœ…

What Does an Embedding Look Like?
An embedding is a vector â€” a list of floating point numbers. Google's gemini-embedding-001 model produces 768 numbers per text:
python"01-Jan Zomato Food Order 850" â†’

[0.023, -0.156, 0.891, 0.234, -0.567, 0.123,
 0.445, -0.234, 0.678, 0.012, -0.345, 0.789,
 ... 768 numbers total ...]
```

Each number captures a different **dimension of meaning** â€” topic, sentiment, context, word relationships.

---

### How Similarity is Measured â€” Cosine Similarity
```
Question embedding:  [0.21, 0.79, 0.11, 0.88]
Chunk 1 embedding:   [0.20, 0.80, 0.10, 0.90] â†’ similarity: 0.99 âœ… very similar
Chunk 2 embedding:   [0.90, 0.10, 0.70, 0.20] â†’ similarity: 0.21 âŒ not similar
Chunks with highest similarity scores are retrieved and sent to Gemini.

Why Google's Embedding Model?
ReasonDetailFreeIncluded with Gemini API keySame providerNo extra accounts neededHigh qualityTrained on massive multilingual dataIndian language supportWorks with Hindi, Tamil etc.
Model used: models/gemini-embedding-001

Note: Original code used models/embedding-001 which gave 404 error. Listed available models and found correct name models/gemini-embedding-001.


ğŸ’» Part 2 â€” Code Created
File: src/embeddings_manager.py
pythonfrom langchain_google_genai import GoogleGenerativeAIEmbeddings
from document_loader import load_document, preprocess_document
from text_chunker import create_chunks
from dotenv import load_dotenv
import os
import time

load_dotenv()

# â”€â”€ Step 1: Create embedding model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_embedding_model():
    """
    Initialize Google's embedding model.
    This model converts text to vectors (lists of numbers).
    """
    print("\nğŸ¤– Initializing embedding model...")

    embedding_model = GoogleGenerativeAIEmbeddings(
        model="models/gemini-embedding-001",
        google_api_key=os.getenv("GEMINI_API_KEY")
    )

    print("âœ… Embedding model ready!")
    return embedding_model


# â”€â”€ Step 2: Test single embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def test_single_embedding(embedding_model):
    """
    Test embedding on a single sentence.
    Shows us what an embedding actually looks like.
    """
    print("\n" + "="*50)
    print("ğŸ”¬ SINGLE EMBEDDING TEST")
    print("="*50)

    test_text = "Zomato food order January"
    embedding = embedding_model.embed_query(test_text)

    print(f"Text: '{test_text}'")
    print(f"Embedding dimensions: {len(embedding)}")
    print(f"First 10 numbers: {[round(x, 4) for x in embedding[:10]]}")
    print(f"Last 10 numbers : {[round(x, 4) for x in embedding[-10:]]}")
    print(f"Min value: {round(min(embedding), 4)}")
    print(f"Max value: {round(max(embedding), 4)}")


# â”€â”€ Step 3: Compare similarity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compare_similarity(embedding_model):
    """
    Compare embeddings of similar and different texts.
    Shows how semantically similar texts have similar embeddings.
    """
    print("\n" + "="*50)
    print("ğŸ” SIMILARITY COMPARISON")
    print("="*50)

    texts = {
        "food_1" : "Zomato food order January",
        "food_2" : "Swiggy restaurant delivery charge",
        "salary" : "Monthly salary bank credit"
    }

    embeddings = {}
    for key, text in texts.items():
        embeddings[key] = embedding_model.embed_query(text)
        print(f"âœ… Embedded: '{text}'")
        time.sleep(1)

    def cosine_similarity(vec1, vec2):
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a ** 2 for a in vec1) ** 0.5
        magnitude2 = sum(b ** 2 for b in vec2) ** 0.5
        return dot_product / (magnitude1 * magnitude2)

    sim_food = cosine_similarity(embeddings["food_1"], embeddings["food_2"])
    sim_diff = cosine_similarity(embeddings["food_1"], embeddings["salary"])

    print(f"\nğŸ“Š Similarity Results:")
    print(f"Zomato vs Swiggy (both food)  : {round(sim_food, 4)} â† should be HIGH")
    print(f"Zomato vs Salary (different)  : {round(sim_diff, 4)} â† should be LOW")
    print(f"\nâœ… Proof that embeddings capture meaning!")


# â”€â”€ Step 4: Embed all chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed_chunks(chunks, embedding_model):
    """
    Generate embeddings for all chunks.
    In real usage ChromaDB does this automatically.
    Here we do it manually to understand the process.
    """
    print("\n" + "="*50)
    print("ğŸ“¦ EMBEDDING ALL CHUNKS")
    print("="*50)

    print(f"Total chunks to embed: {len(chunks)}")

    embedded_chunks = []

    for i, chunk in enumerate(chunks):
        embedding = embedding_model.embed_query(chunk.page_content)
        embedded_chunks.append({
            "chunk_number" : i + 1,
            "content"      : chunk.page_content,
            "metadata"     : chunk.metadata,
            "embedding"    : embedding,
            "dimensions"   : len(embedding)
        })
        print(f"âœ… Chunk {i+1}/{len(chunks)} embedded ({len(embedding)} dimensions)")
        time.sleep(1)

    print(f"\nâœ… All {len(chunks)} chunks embedded successfully!")
    return embedded_chunks


# â”€â”€ Step 5: Inspect embedded chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def inspect_embedded_chunks(embedded_chunks):
    """
    Show summary of embedded chunks.
    """
    print("\n" + "="*50)
    print("ğŸ“‹ EMBEDDED CHUNKS SUMMARY")
    print("="*50)

    for item in embedded_chunks:
        print(f"\n--- Chunk {item['chunk_number']} ---")
        print(f"Content preview : {item['content'][:80]}...")
        print(f"Embedding dims  : {item['dimensions']}")
        print(f"First 5 numbers : {[round(x, 4) for x in item['embedding'][:5]]}")


# â”€â”€ Main: Run all steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":

    embedding_model = create_embedding_model()
    test_single_embedding(embedding_model)
    compare_similarity(embedding_model)

    file_path = "data/sample_statement.txt"
    documents = load_document(file_path)
    cleaned_docs = preprocess_document(documents)
    chunks = create_chunks(cleaned_docs)

    embedded_chunks = embed_chunks(chunks, embedding_model)
    inspect_embedded_chunks(embedded_chunks)

    print("\nâœ… Embeddings complete!")
    print("ğŸ”œ Next step: Store in ChromaDB (Day 8)")

ğŸ” Part 3 â€” Detailed Code Explanation
Imports
pythonfrom langchain_google_genai import GoogleGenerativeAIEmbeddings
from document_loader import load_document, preprocess_document
from text_chunker import create_chunks
from dotenv import load_dotenv
import os
import time
ImportPurposeGoogleGenerativeAIEmbeddingsLangChain wrapper for Google's embedding modelload_document, preprocess_documentOur Day 5 functions â€” reused herecreate_chunksOur Day 6 function â€” reused hereosRead environment variablestimetime.sleep() to avoid hitting API rate limits
Key concept â€” reusing our own code:
pythonfrom document_loader import load_document, preprocess_document
from text_chunker import create_chunks
We are building a pipeline â€” each day's code builds on the previous day. Instead of rewriting, we import and reuse. This is professional software development practice.

Function 1 â€” create_embedding_model()
pythonembedding_model = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001",
    google_api_key=os.getenv("GEMINI_API_KEY")
)
GoogleGenerativeAIEmbeddings â€” LangChain's wrapper class for Google's embedding API. Handles all the complexity of making API calls and returning vectors.
model="models/gemini-embedding-001" â€” Specifies which Google embedding model to use. This model was found by listing available models:
pythonfor model in genai.list_models():
    if 'embedContent' in model.supported_generation_methods:
        print(model.name)
Why store as variable and return?
pythonembedding_model = create_embedding_model()
We create the model once and reuse it for all embedding operations. Creating it multiple times would be wasteful â€” each creation makes an API connection.

Function 2 â€” test_single_embedding()
pythontest_text = "Zomato food order January"
embedding = embedding_model.embed_query(test_text)
embed_query() â€” converts a single text string into a vector. Returns a Python list of floating point numbers.
pythonprint(f"Embedding dimensions: {len(embedding)}")
len(embedding) counts the numbers in the vector. Google's model always returns 768 dimensions.
pythonprint(f"First 10 numbers: {[round(x, 4) for x in embedding[:10]]}")
round(x, 4) â†’ rounds each number to 4 decimal places for readable output.
embedding[:10] â†’ first 10 numbers from the 768-number vector.
[round(x, 4) for x in ...] â†’ list comprehension applying rounding to each number.
pythonprint(f"Min value: {round(min(embedding), 4)}")
print(f"Max value: {round(max(embedding), 4)}")
Embedding values typically range between -1 and +1. Negative values are as meaningful as positive â€” they represent different directions in the meaning space.

Function 3 â€” compare_similarity()
pythontexts = {
    "food_1" : "Zomato food order January",
    "food_2" : "Swiggy restaurant delivery charge",
    "salary" : "Monthly salary bank credit"
}
Dictionary â€” stores key-value pairs. Key is our label ("food_1"), value is the text. Makes it easy to reference each text by name.
pythonembeddings = {}
for key, text in texts.items():
    embeddings[key] = embedding_model.embed_query(text)
    print(f"âœ… Embedded: '{text}'")
    time.sleep(1)
texts.items() â†’ loops through dictionary giving both key and value at the same time.
embeddings[key] = ... â†’ stores each embedding in a new dictionary using same key.
time.sleep(1) â†’ pauses 1 second between API calls to avoid rate limit errors.
After this loop:
pythonembeddings = {
    "food_1": [0.2, 0.8, 0.1, ...],   # 768 numbers
    "food_2": [0.21, 0.79, 0.12, ...], # 768 numbers
    "salary": [0.9, 0.1, 0.7, ...]     # 768 numbers
}
Cosine Similarity function:
pythondef cosine_similarity(vec1, vec2):
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = sum(a ** 2 for a in vec1) ** 0.5
    magnitude2 = sum(b ** 2 for b in vec2) ** 0.5
    return dot_product / (magnitude1 * magnitude2)
This is a nested function â€” a function defined inside another function. It's only accessible within compare_similarity().
Breaking down the math:
zip(vec1, vec2) â€” pairs up numbers from both vectors:
pythonvec1 = [0.2, 0.8, 0.1]
vec2 = [0.21, 0.79, 0.12]
zip  = [(0.2, 0.21), (0.8, 0.79), (0.1, 0.12)]
dot_product â€” multiplies paired numbers and sums them:
python(0.2 Ã— 0.21) + (0.8 Ã— 0.79) + (0.1 Ã— 0.12) = 0.686
magnitude â€” length of the vector:
pythonsum(a ** 2 for a in vec1) ** 0.5
= (0.2Â² + 0.8Â² + 0.1Â²) ** 0.5
= (0.04 + 0.64 + 0.01) ** 0.5
= 0.69 ** 0.5
= 0.831
** 0.5 means square root (0.5 power = square root).
Final result:
pythonreturn dot_product / (magnitude1 * magnitude2)
Result is always between -1 and 1:

1.0 â†’ identical meaning
0.8+ â†’ very similar meaning
0.5 â†’ somewhat related
0.2- â†’ very different meaning


Function 4 â€” embed_chunks()
pythonembedded_chunks = []

for i, chunk in enumerate(chunks):
    embedding = embedding_model.embed_query(chunk.page_content)
    embedded_chunks.append({
        "chunk_number" : i + 1,
        "content"      : chunk.page_content,
        "metadata"     : chunk.metadata,
        "embedding"    : embedding,
        "dimensions"   : len(embedding)
    })
    time.sleep(1)
embedded_chunks.append({...}) â€” adds a dictionary to the list for each chunk. Each dictionary contains all information about that chunk including its embedding.
Why store everything in a dictionary?
Keeps all related data together:
python{
    "chunk_number": 1,
    "content"     : "HDFC BANK - ACCOUNT STATEMENT...",
    "metadata"    : {"source": "data/sample_statement.txt"},
    "embedding"   : [0.023, -0.156, 0.891, ...],  # 768 numbers
    "dimensions"  : 768
}
Important note: In Day 8 with ChromaDB we won't need to manually embed chunks â€” ChromaDB does it automatically. We do it manually here purely to understand what's happening under the hood.

Function 5 â€” inspect_embedded_chunks()
pythonprint(f"Content preview : {item['content'][:80]}...")
print(f"Embedding dims  : {item['dimensions']}")
print(f"First 5 numbers : {[round(x, 4) for x in item['embedding'][:5]]}")
```

`item['content'][:80]` â†’ first 80 characters of chunk content as preview.
`item['embedding'][:5]` â†’ first 5 numbers from 768-number embedding.
`...` at the end â†’ visually indicates content is truncated.

---

## ğŸ—ºï¸ RAG Pipeline Progress
```
âœ… Step 1: Load Document       â† Day 5
âœ… Step 2: Preprocess Text     â† Day 5
âœ… Step 3: Split into Chunks   â† Day 6
âœ… Step 4: Create Embeddings   â† Done today!
â³ Step 5: Store in ChromaDB   â† Day 8
â³ Step 6: Query & Retrieve    â† Day 9
â³ Step 7: Generate Answer     â† Day 9
â³ Step 8: Integrate into UI   â† Day 10

ğŸ’¡ Key Python Concepts Learned
ConceptExampleMeaningDictionary{"key": "value"}Store key-value pairsdict.items()for k, v in dict.items()Loop through key and value togetherNested functiondef func() inside def func()Function only accessible within parent functionzip()zip(vec1, vec2)Pair up elements from two lists** 0.5value ** 0.5Square root** 2value ** 2Square (power of 2)time.sleep(1)Pause 1 secondAvoid API rate limit errorsround(x, 4)round(0.12345, 4)Round to 4 decimal places

âš ï¸ Issues Faced & Solutions
IssueSolution404 NOT_FOUND models/embedding-001Listed available embedding models using genai.list_models(). Found correct model name models/gemini-embedding-001 and updated code

âœ… Day 7 Checklist

 Understand what embeddings are and why they work
 Understand what a vector is and its dimensions
 Understand cosine similarity concept and math
 Created src/embeddings_manager.py
 Fixed embedding model name from embedding-001 to gemini-embedding-001
 Tested single embedding â€” confirmed 768 dimensions
 Compared similarity â€” food texts scored HIGH, salary scored LOW
 Embedded all chunks manually to understand the process
 Committed and pushed to GitHub
